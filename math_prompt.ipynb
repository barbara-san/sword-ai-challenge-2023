{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_KEY = \"4f8e9d40be39474c96fa1327dbd47516\"\n",
    "AZURE_OPENAI_ENDPOINT = \"https://openai-resource-team-3-france.openai.azure.com/\"\n",
    "OPENAI_API_VERSION = \"2023-03-15-preview\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = AZURE_OPENAI_KEY\n",
    "os.environ[\"OPENAI_API_BASE\"] = AZURE_OPENAI_ENDPOINT\n",
    "os.environ[\"OPENAI_API_VERSION\"] = OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_llm = AzureChatOpenAI(deployment_name=\"gpt35-team-3-0301\", max_tokens=100, temperature=0.2)\n",
    "summary_llm = AzureChatOpenAI(deployment_name=\"gpt35-team-3-0301\")\n",
    "# text = \"How to be happy?\"\n",
    "# print(chatgpt([HumanMessage(content=text)]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, CombinedMemory, ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to create a base pre-prompt for math\n",
    "context = \"\"\"\\\n",
    "The following text is simply an guide that the AI must follow after receiving this text input. \\\n",
    "The AI MUST asnwer to this prompt with \\\"Understood!\\\" and nothing else, as it only exists to give context to the AI. \\\n",
    "This is a friendly conversation between a human and an AI. \\\n",
    "The AI serves the purpose of being a mathematics tutor. \\\n",
    "The human is considered to be a highschooler learning in the Portuguese schools, which follow the portuguese teaching system. \\\n",
    "The AI should help the human by giving clear detail on how to solve an exercise or how to understand a concept. \\\n",
    "The AI must not use techniques that are considered to be of higher educational experience. \\\n",
    "Any equations provided by the AI should be written delimited by ```. \\\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history_lines\",\n",
    "    input_key=\"input\",\n",
    "    k=1\n",
    ")\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm=summary_llm, input_key=\"input\")\n",
    "# Combined\n",
    "memory = CombinedMemory(memories=[conv_memory, summary_memory])\n",
    "\n",
    "_DEFAULT_TEMPLATE = f\"{context}\" + \"\"\"\n",
    "\n",
    "Summary of conversation:\n",
    "{history}\n",
    "Current conversation:\n",
    "{chat_history_lines}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\", \"chat_history_lines\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=math_llm, \n",
    "    verbose=True, # change to False to only show the answer when running\n",
    "    memory=memory,\n",
    "    prompt=PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving the context to the bot\n",
    "conversation.run(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.run(\"What are steps to find the global minimum in a function?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.run(\"Could you keep explaining please?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.run(\"Keep going!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.run(\"Keep going!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
